{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generator (Object Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conf\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bounding box map (from GDT-HWD to AWS format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"left\": xmin\n",
    "    \"top\": ymin\n",
    "    \"width\": xmax - xmin\n",
    "    \"height\": yman - ymin\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"none\": 0\n",
    "    \"red\": 1\n",
    "    \"yellow\": 2\n",
    "    \"white\": 3\n",
    "    \"blue\": 4\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step transformations (concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf train train_annotation validation validation_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir train train_annotation validation validation_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. read all annotations ordered (**GDUT-HWD/Annotations**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "raw_annotation_path = conf.raw_annotation_path\n",
    "annotations = os.listdir(raw_annotation_path)\n",
    "\n",
    "annotations.sort()\n",
    "jpgs = [x.replace('xml', 'jpg') for x in annotations]\n",
    "jsons = [x.replace('jpg', 'json') for x in annotations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split 80% to train e 20% to test\n",
    "\n",
    "- Where X are the jpgs and Y are the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_annotation, validation_annotation, train_jpgs, validation_jpgs = train_test_split(annotations,\n",
    "                                                                                        jpgs,\n",
    "                                                                                        test_size=0.2,\n",
    "                                                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer!\n",
    "\n",
    "You should update in conf.py the following variables:\n",
    "\n",
    "- num_training_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEY! num_training_samples to 2540 in conf.py\n"
     ]
    }
   ],
   "source": [
    "num_training_samples = len(train_annotation)\n",
    "print('HEY! num_training_samples to {} in conf.py'.format(num_training_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Annotation from XML to Json in the template.json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Generate train_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_annotation(path, annotation_list):\n",
    "    for filename in annotation_list:\n",
    "        if filename != '.ipynb_checkpoints':\n",
    "            image_id = filename.split('.')[0]\n",
    "            root = ET.parse(conf.raw_annotation_path + '/{}'.format(filename)).getroot()\n",
    "\n",
    "            annotations = []\n",
    "            categories = []\n",
    "\n",
    "            for size in root.findall('size'):\n",
    "                W = size.find('width').text\n",
    "                H = size.find('height').text\n",
    "                depth = size.find('depth').text\n",
    "\n",
    "\n",
    "\n",
    "            for ob in root.findall('object'):\n",
    "                name = ob.find('name').text\n",
    "                bndbox = ob.find('bndbox')\n",
    "                xmin = int(bndbox.find('xmin').text)\n",
    "                ymin = int(bndbox.find('ymin').text)\n",
    "                xmax = int(bndbox.find('xmax').text)\n",
    "                ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "                left = xmin\n",
    "                top = ymin\n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "\n",
    "                annotation = helper.generate_annotation(name, left, top, width, height)\n",
    "                categorie =  helper.generate_categorie(name, conf.object_categories)\n",
    "\n",
    "                annotations.append(annotation)\n",
    "                categories.append(categorie)\n",
    "\n",
    "            categories = list({c['class_id']:c for c in categories}.values())\n",
    "\n",
    "            data = helper.generate_annotation_file_dict(W, H, depth, image_id, annotations, categories)\n",
    "            helper.dict_to_json(path+'_annotation', image_id, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "generate_annotation('train', train_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "generate_annotation('validation', validation_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. copy imagem from Its folder (train or validation)\n",
    "    \n",
    "    - Template must be: \"<folder>/<id>.jpg\"\n",
    "        - folder: train or validation\n",
    "        - id: image name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "for filename in train_annotation:\n",
    "    im = filename.replace('xml', 'jpg')\n",
    "    resp = ! cp GDUT-HWD/JPEGImages/$im train/\n",
    "    \n",
    "\n",
    "for filename in validation_annotation:\n",
    "    im = filename.replace('xml', 'jpg')\n",
    "    resp = ! cp GDUT-HWD/JPEGImages/$im validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Move All folders to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "resp = ! aws s3 cp train s3://hardhat-dataset-sagemaker-object-detector-solvimm/train --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "resp = ! aws s3 cp validation s3://hardhat-dataset-sagemaker-object-detector-solvimm/validation --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "resp = ! aws s3 cp train_annotation s3://hardhat-dataset-sagemaker-object-detector-solvimm/train_annotation --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "resp = ! aws s3 cp validation_annotation s3://hardhat-dataset-sagemaker-object-detector-solvimm/validation_annotation --recursive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
